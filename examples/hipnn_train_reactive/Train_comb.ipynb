{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5478a27-32c6-40fd-b8fc-b5481f1133af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(50000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/envs/hipnn_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decorating your function! <function KSA_XL_BOMD.one_step at 0x7f538ba65550>\n"
     ]
    }
   ],
   "source": [
    "# to save output in log file #\n",
    "# ---------------------------- #\n",
    "##############################################\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"train1.log\", \"w+\")\n",
    "\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 50\n",
    "##############################################\n",
    "\n",
    "\n",
    "# ---------------- #\n",
    "# Imported Modules #\n",
    "# ---------------- #\n",
    "import os\n",
    "import sys\n",
    "### path to PYSEQM ###\n",
    "sys.path.insert(1, \"/home/maxim/Projects/git2/PYSEQM_dev/\")\n",
    "#sys.path.insert(1, '/home/maxim/Projects/pyseqm_d/My_d_combined/PYSEQM_dev/')\n",
    "\n",
    "\n",
    "### path to HIPNN ###\n",
    "sys.path.append('/home/maxim/Projects/hipnn/hippynn_restricted/')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from hippynn.interfaces.pyseqm_interface.seqm_nodes import *\n",
    "from hippynn.interfaces.pyseqm_interface.callback import update_scf_eps, save_and_stop_after\n",
    "import hippynn.interfaces.pyseqm_interface\n",
    "import hippynn\n",
    "from hippynn.graphs import inputs, networks, targets, physics\n",
    "from hippynn.graphs import loss\n",
    "from hippynn import plotting\n",
    "from hippynn.databases import DirectoryDatabase\n",
    "from hippynn.experiment.assembly import assemble_for_training\n",
    "from hippynn.experiment.controllers import RaiseBatchSizeOnPlateau,PatienceController\n",
    "from hippynn.experiment import setup_training\n",
    "from hippynn.experiment import train_model\n",
    "import seqm\n",
    "from seqm.basics import parameterlist\n",
    "\n",
    "from numba import cuda\n",
    "### keeps SCF loops silent ###\n",
    "seqm.seqm_functions.scf_loop.debug = False\n",
    "\n",
    "hippynn.interfaces.pyseqm_interface.check.debug = True\n",
    "\n",
    "### maximum allowed SCF iterations ###\n",
    "seqm.seqm_functions.scf_loop.MAX_ITER = 95\n",
    "\n",
    "# torch.cuda.set_device(0) # Don't try this if you want CPU training!\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac3761-282d-4964-983f-cc4254c83f04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined Inputs: [\"Species(db_name='Z')\", \"Species(db_name='Z')-true\", 'SEQM_Energy.mol_energy-true', \"Positions(db_name='R')\"]\n",
      "Determined Outputs: ['L^P_Reg(HIPNN_seqm,p=2)', 'SEQM_Atom_Params.atom_charges', 'SEQM_MaskMolAtom_Pred', 'PeratomTrue', 'Atom_Mask', 'Scale', 'SEQM_MolMask', 'SEQM_MaskMol_Pred', 'SEQM_PerAtom_Pred']\n",
      "Determined Targets: ['gradients', 'SEQM_Energy.mol_energy']\n",
      "Device was not specified. Attempting to default to device: cuda:0\n",
      "Inputs:\n",
      "\t I0 : SpeciesNode('Species(db_name='Z')')<0x7f538b98a610>\n",
      "\t I1 : LossTrueNode('Species(db_name='Z')-true')<0x7f538b9c90a0>\n",
      "\t I2 : LossTrueNode('SEQM_Energy.mol_energy-true')<0x7f538b9bc6d0>\n",
      "\t I3 : PositionsNode('Positions(db_name='R')')<0x7f538b98a5e0>\n",
      "Outputs:\n",
      "\t O0 : _LPReg('L^P_Reg(HIPNN_seqm,p=2)')<0x7f538b9d2a00>\n",
      "\t O1 : IndexNode('SEQM_Atom_Params.atom_charges')<0x7f538b9a1a00>\n",
      "\t O2 : SEQM_MaskOnMolAtomNode('SEQM_MaskMolAtom_Pred')<0x7f538b9bca30>\n",
      "\t O3 : PerAtom('PeratomTrue')<0x7f538b9c9070>\n",
      "\t O4 : AtomMaskNode('Atom_Mask')<0x7f538b9bca90>\n",
      "\t O5 : ScaleNode('Scale')<0x7f538b98b0d0>\n",
      "\t O6 : SEQM_MolMaskNode('SEQM_MolMask')<0x7f538b9bcb50>\n",
      "\t O7 : SEQM_MaskOnMolNode('SEQM_MaskMol_Pred')<0x7f538b9bc760>\n",
      "\t O8 : SEQM_MaskOnMolNode('SEQM_PerAtom_Pred')<0x7f538b994fa0>\n",
      "Order:\n",
      "I0-------------------> O4  : Atom_Mask\n",
      "I2-------------------> H1  : Atleast2D(LossTrueNode('SEQM_Energy.mol_energy-true')<0x7f538b9bc6d0>)\n",
      "H1,I1----------------> O3  : PeratomTrue\n",
      "I0-------------------> H3  : OneHot\n",
      "H3-------------------> H4  : OneHot.encoding\n",
      "H3-------------------> H5  : OneHot.nonblank\n",
      "H5,H4----------------> H6  : PaddingIndexer\n",
      "H6-------------------> H7  : PaddingIndexer.n_atoms_max\n",
      "H6-------------------> H8  : PaddingIndexer.indexed_features\n",
      "H6-------------------> H9  : PaddingIndexer.real_atoms\n",
      "H6-------------------> H10 : PaddingIndexer.inv_real_atoms\n",
      "H6-------------------> H11 : PaddingIndexer.mol_index\n",
      "H6-------------------> H12 : PaddingIndexer.atom_index\n",
      "I3,H5,H9,H10---------> H13 : PairIndexer\n",
      "H13------------------> H14 : PairIndexer.pair_dist\n",
      "H13------------------> H15 : PairIndexer.pair_first\n",
      "H13------------------> H16 : PairIndexer.pair_second\n",
      "H13------------------> H17 : PairIndexer.pair_coord\n",
      "H16,H8,H14,H15-------> H18 : HIPNN_seqm\n",
      "H18------------------> O0  : L^P_Reg(HIPNN_seqm,p=2)\n",
      "H18------------------> H20 : SEQM_Atom_Params\n",
      "H20------------------> O1  : SEQM_Atom_Params.atom_charges\n",
      "H20------------------> H22 : SEQM_Atom_Params.partial_sums\n",
      "I3,O1,I0-------------> H23 : SEQM_Energy\n",
      "H23------------------> H24 : SEQM_Energy.atomic_charge\n",
      "H23------------------> H25 : SEQM_Energy.notconverged\n",
      "H25------------------> O5  : Scale\n",
      "H25------------------> O6  : SEQM_MolMask\n",
      "H20------------------> H28 : SEQM_Atom_Params.charge_hierarchality\n",
      "H23------------------> H29 : SEQM_Energy.orbital_charges\n",
      "H23------------------> H30 : SEQM_Energy.isolated_atom_energy\n",
      "H23------------------> H31 : SEQM_Energy.nuclear_energy\n",
      "H23------------------> H32 : SEQM_Energy.electric_energy\n",
      "H23------------------> H33 : SEQM_Energy.single_particle_density_matrix\n",
      "H23------------------> H34 : SEQM_Energy.orbital_energies\n",
      "H23------------------> H35 : SEQM_Energy.Etot_m_Eiso\n",
      "H6-------------------> H36 : PaddingIndexer.n_molecules\n",
      "H23------------------> H37 : SEQM_Energy.mol_energy\n",
      "I0,H37---------------> H38 : PeratomPredicted\n",
      "I3,H37---------------> H39 : gradients\n",
      "O6,H39,O4------------> O2  : SEQM_MaskMolAtom_Pred\n",
      "O6,H37---------------> O7  : SEQM_MaskMol_Pred\n",
      "O6,H38---------------> O8  : SEQM_PerAtom_Pred\n",
      "Arrays found:  {'F_PM6': 'F_PM6.npy', 'R': 'R.npy', 'Z': 'Z.npy', 'F_DFT': 'F_DFT.npy', 'Etot_PM6': 'Etot_PM6.npy', 'Q': 'Q.npy', 'Eatomiz_PM6': 'Eatomiz_PM6.npy', 'Etot_DFT': 'Etot_DFT.npy', 'Et_mNONd3h4bias': 'Et_mNONd3h4bias.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/envs/hipnn_1/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "{'R': dtype('float64'), 'Z': dtype('int64'), 'F_DFT': dtype('float64'), 'Et_mNONd3h4bias': dtype('float64')}\n",
      "All arrays:\n",
      "--------------------------------------------------------------------------------------\n",
      "| Name               | dtype              | shape                                    |\n",
      "--------------------------------------------------------------------------------------\n",
      "| R                  | dtype('float64')   | (149415, 20, 3)                          |\n",
      "| Z                  | dtype('int64')     | (149415, 20)                             |\n",
      "| F_DFT              | dtype('float64')   | (149415, 20, 3)                          |\n",
      "| Et_mNONd3h4bias    | dtype('float64')   | (149415,)                                |\n",
      "--------------------------------------------------------------------------------------\n",
      "Database: Using auto-generated data indices\n",
      "Arrays for split: ignore\n",
      "--------------------------------------------------------------------------------------\n",
      "| Name               | dtype              | shape                                    |\n",
      "--------------------------------------------------------------------------------------\n",
      "| R                  | torch.float64      | torch.Size([104590, 20, 3])              |\n",
      "| Z                  | torch.int64        | torch.Size([104590, 20])                 |\n",
      "| F_DFT              | torch.float64      | torch.Size([104590, 20, 3])              |\n",
      "| Et_mNONd3h4bias    | torch.float64      | torch.Size([104590])                     |\n",
      "| indices            | torch.int64        | torch.Size([104590])                     |\n",
      "| split_indices      | torch.int64        | torch.Size([104590])                     |\n",
      "--------------------------------------------------------------------------------------\n",
      "Arrays for split: test\n",
      "--------------------------------------------------------------------------------------\n",
      "| Name               | dtype              | shape                                    |\n",
      "--------------------------------------------------------------------------------------\n",
      "| R                  | torch.float64      | torch.Size([4482, 20, 3])                |\n",
      "| Z                  | torch.int64        | torch.Size([4482, 20])                   |\n",
      "| F_DFT              | torch.float64      | torch.Size([4482, 20, 3])                |\n",
      "| Et_mNONd3h4bias    | torch.float64      | torch.Size([4482])                       |\n",
      "| indices            | torch.int64        | torch.Size([4482])                       |\n",
      "| split_indices      | torch.int64        | torch.Size([4482])                       |\n",
      "--------------------------------------------------------------------------------------\n",
      "Arrays for split: valid\n",
      "--------------------------------------------------------------------------------------\n",
      "| Name               | dtype              | shape                                    |\n",
      "--------------------------------------------------------------------------------------\n",
      "| R                  | torch.float64      | torch.Size([4482, 20, 3])                |\n",
      "| Z                  | torch.int64        | torch.Size([4482, 20])                   |\n",
      "| F_DFT              | torch.float64      | torch.Size([4482, 20, 3])                |\n",
      "| Et_mNONd3h4bias    | torch.float64      | torch.Size([4482])                       |\n",
      "| indices            | torch.int64        | torch.Size([4482])                       |\n",
      "| split_indices      | torch.int64        | torch.Size([4482])                       |\n",
      "--------------------------------------------------------------------------------------\n",
      "Arrays for split: train\n",
      "--------------------------------------------------------------------------------------\n",
      "| Name               | dtype              | shape                                    |\n",
      "--------------------------------------------------------------------------------------\n",
      "| R                  | torch.float64      | torch.Size([35861, 20, 3])               |\n",
      "| Z                  | torch.int64        | torch.Size([35861, 20])                  |\n",
      "| F_DFT              | torch.float64      | torch.Size([35861, 20, 3])               |\n",
      "| Et_mNONd3h4bias    | torch.float64      | torch.Size([35861])                      |\n",
      "| indices            | torch.int64        | torch.Size([35861])                      |\n",
      "| split_indices      | torch.int64        | torch.Size([35861])                      |\n",
      "--------------------------------------------------------------------------------------\n",
      "SetupParams(device='cuda', controller=<hippynn.experiment.controllers.PatienceController object at 0x7f538b8e1700>, stopping_key=None, optimizer=<class 'torch.optim.adam.Adam'>, learning_rate=None, scheduler=None, batch_size=None, eval_batch_size=None, max_epochs=None, fraction_train_eval=0.1)\n",
      "Using device:  cuda\n",
      "Beginning training.\n",
      "Model:\n",
      "Inputs:\n",
      "\t I0 : SpeciesNode('Species(db_name='Z')')<0x7f538b98a610>\n",
      "\t I1 : LossTrueNode('Species(db_name='Z')-true')<0x7f538b9c90a0>\n",
      "\t I2 : LossTrueNode('SEQM_Energy.mol_energy-true')<0x7f538b9bc6d0>\n",
      "\t I3 : PositionsNode('Positions(db_name='R')')<0x7f538b98a5e0>\n",
      "Outputs:\n",
      "\t O0 : _LPReg('L^P_Reg(HIPNN_seqm,p=2)')<0x7f538b9d2a00>\n",
      "\t O1 : IndexNode('SEQM_Atom_Params.atom_charges')<0x7f538b9a1a00>\n",
      "\t O2 : SEQM_MaskOnMolAtomNode('SEQM_MaskMolAtom_Pred')<0x7f538b9bca30>\n",
      "\t O3 : PerAtom('PeratomTrue')<0x7f538b9c9070>\n",
      "\t O4 : AtomMaskNode('Atom_Mask')<0x7f538b9bca90>\n",
      "\t O5 : ScaleNode('Scale')<0x7f538b98b0d0>\n",
      "\t O6 : SEQM_MolMaskNode('SEQM_MolMask')<0x7f538b9bcb50>\n",
      "\t O7 : SEQM_MaskOnMolNode('SEQM_MaskMol_Pred')<0x7f538b9bc760>\n",
      "\t O8 : SEQM_MaskOnMolNode('SEQM_PerAtom_Pred')<0x7f538b994fa0>\n",
      "Order:\n",
      "I0-------------------> O4  : Atom_Mask\n",
      "I2-------------------> H1  : Atleast2D(LossTrueNode('SEQM_Energy.mol_energy-true')<0x7f538b9bc6d0>)\n",
      "H1,I1----------------> O3  : PeratomTrue\n",
      "I0-------------------> H3  : OneHot\n",
      "H3-------------------> H4  : OneHot.encoding\n",
      "H3-------------------> H5  : OneHot.nonblank\n",
      "H5,H4----------------> H6  : PaddingIndexer\n",
      "H6-------------------> H7  : PaddingIndexer.n_atoms_max\n",
      "H6-------------------> H8  : PaddingIndexer.indexed_features\n",
      "H6-------------------> H9  : PaddingIndexer.real_atoms\n",
      "H6-------------------> H10 : PaddingIndexer.inv_real_atoms\n",
      "H6-------------------> H11 : PaddingIndexer.mol_index\n",
      "H6-------------------> H12 : PaddingIndexer.atom_index\n",
      "I3,H5,H9,H10---------> H13 : PairIndexer\n",
      "H13------------------> H14 : PairIndexer.pair_dist\n",
      "H13------------------> H15 : PairIndexer.pair_first\n",
      "H13------------------> H16 : PairIndexer.pair_second\n",
      "H13------------------> H17 : PairIndexer.pair_coord\n",
      "H16,H8,H14,H15-------> H18 : HIPNN_seqm\n",
      "H18------------------> O0  : L^P_Reg(HIPNN_seqm,p=2)\n",
      "H18------------------> H20 : SEQM_Atom_Params\n",
      "H20------------------> O1  : SEQM_Atom_Params.atom_charges\n",
      "H20------------------> H22 : SEQM_Atom_Params.partial_sums\n",
      "I3,O1,I0-------------> H23 : SEQM_Energy\n",
      "H23------------------> H24 : SEQM_Energy.atomic_charge\n",
      "H23------------------> H25 : SEQM_Energy.notconverged\n",
      "H25------------------> O5  : Scale\n",
      "H25------------------> O6  : SEQM_MolMask\n",
      "H20------------------> H28 : SEQM_Atom_Params.charge_hierarchality\n",
      "H23------------------> H29 : SEQM_Energy.orbital_charges\n",
      "H23------------------> H30 : SEQM_Energy.isolated_atom_energy\n",
      "H23------------------> H31 : SEQM_Energy.nuclear_energy\n",
      "H23------------------> H32 : SEQM_Energy.electric_energy\n",
      "H23------------------> H33 : SEQM_Energy.single_particle_density_matrix\n",
      "H23------------------> H34 : SEQM_Energy.orbital_energies\n",
      "H23------------------> H35 : SEQM_Energy.Etot_m_Eiso\n",
      "H6-------------------> H36 : PaddingIndexer.n_molecules\n",
      "H23------------------> H37 : SEQM_Energy.mol_energy\n",
      "I0,H37---------------> H38 : PeratomPredicted\n",
      "I3,H37---------------> H39 : gradients\n",
      "O6,H39,O4------------> O2  : SEQM_MaskMolAtom_Pred\n",
      "O6,H37---------------> O7  : SEQM_MaskMol_Pred\n",
      "O6,H38---------------> O8  : SEQM_PerAtom_Pred\n",
      "Model Params:\n",
      "cuda:0 Fixed torch.int64 torch.Size([9]) moddict.node3.species_map\n",
      "cuda:0 Learned torch.float64 torch.Size([18, 64, 4]) moddict.node18.blocks.0.0.base_layer.int_weights\n",
      "cuda:0 Learned torch.float64 torch.Size([1, 18]) moddict.node18.blocks.0.0.base_layer.sensitivity.mu\n",
      "cuda:0 Learned torch.float64 torch.Size([1, 18]) moddict.node18.blocks.0.0.base_layer.sensitivity.sigma\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 4]) moddict.node18.blocks.0.0.base_layer.selfint.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.0.base_layer.selfint.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.0.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.0.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 4]) moddict.node18.blocks.0.0.adjust_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.1.base_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.1.base_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.1.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.1.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.2.base_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.2.base_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.2.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.2.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.3.base_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.3.base_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.0.3.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.0.3.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([18, 64, 64]) moddict.node18.blocks.1.0.base_layer.int_weights\n",
      "cuda:0 Learned torch.float64 torch.Size([1, 18]) moddict.node18.blocks.1.0.base_layer.sensitivity.mu\n",
      "cuda:0 Learned torch.float64 torch.Size([1, 18]) moddict.node18.blocks.1.0.base_layer.sensitivity.sigma\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.0.base_layer.selfint.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.0.base_layer.selfint.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.0.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.0.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.1.base_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.1.base_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.1.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.1.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.2.base_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.2.base_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.2.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.2.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.3.base_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.3.base_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([64, 64]) moddict.node18.blocks.1.3.res_layer.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([64]) moddict.node18.blocks.1.3.res_layer.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([15, 4]) moddict.node20.layers.0.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([15]) moddict.node20.layers.0.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([15, 64]) moddict.node20.layers.1.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([15]) moddict.node20.layers.1.bias\n",
      "cuda:0 Learned torch.float64 torch.Size([15, 64]) moddict.node20.layers.2.weight\n",
      "cuda:0 Learned torch.float64 torch.Size([15]) moddict.node20.layers.2.bias\n",
      "cuda:0 Fixed torch.float64 torch.Size([9, 15]) moddict.node23.p\n",
      "cuda:0 Fixed torch.float64 torch.Size([15]) moddict.node23.weight\n",
      "cuda:0 Fixed torch.float64 torch.Size([9, 15]) moddict.node23.energy.packpar.p\n",
      "cuda:0 Fixed torch.float64 torch.Size([9, 9]) moddict.node23.energy.packpar.alpha\n",
      "cuda:0 Fixed torch.float64 torch.Size([9, 9]) moddict.node23.energy.packpar.chi\n",
      "cuda:0 Fixed torch.float64 torch.Size([]) moddict.node23.energy.hamiltonian.eps\n",
      "cuda:0 Fixed torch.float64 torch.Size([]) moddict.node23.energy.hamiltonian.scf_backward_eps\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.atomic_num\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.tore\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.iso\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.qn\n",
      "cuda:0 Fixed torch.int64 torch.Size([73]) moddict.node23.const.qn_int\n",
      "cuda:0 Fixed torch.int64 torch.Size([73]) moddict.node23.const.qnD_int\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.ussc\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.uppc\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.gssc\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.gspc\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.hspc\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.gp2c\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.gppc\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.eheat\n",
      "cuda:0 Fixed torch.float64 torch.Size([73]) moddict.node23.const.mass\n",
      "Total Count: 144962\n",
      "At least 20 epochs will be run\n",
      "__________________________________________________\n",
      "Epoch 0:\n",
      "Learning rate:    0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   0%|          | 0/598 [00:00<?, ?batch/s]/home/maxim/Projects/git2/PYSEQM_dev/seqm/seqm_functions/make_dm_guess.py:291: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  v_lumo = v[:,0].gather(2, molecule.nocc[:,0].unsqueeze(0).unsqueeze(0).T.repeat(1,v.shape[-1],1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/Projects/git2/PYSEQM_dev/seqm/seqm_functions/scf_loop.py:1386: UserWarning: SCF for 2/60 molecules doesn't converge after 95 iterations\n",
      "  warnings.warn(\"SCF for %d/%d molecules doesn't converge after %d iterations\" % (nnot, nmol, MAX_ITER))\n",
      "Training Batches:   1%|          | 6/598 [03:10<5:08:57, 31.31s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/Projects/git2/PYSEQM_dev/seqm/seqm_functions/scf_loop.py:1386: UserWarning: SCF for 1/60 molecules doesn't converge after 95 iterations\n",
      "  warnings.warn(\"SCF for %d/%d molecules doesn't converge after %d iterations\" % (nnot, nmol, MAX_ITER))\n",
      "Training Batches:   2%|▏         | 10/598 [05:17<5:05:31, 31.18s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   2%|▏         | 12/598 [06:18<5:01:21, 30.86s/batch]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########\n",
    "current_dir = \"/home/maxim/Projects/TESTS/pyseqm_tests/HIPNN_TRAIN/comb_t1x_ani_bse/\"\n",
    "os.chdir(current_dir)\n",
    "\n",
    "def main():\n",
    "\n",
    "    ### directory csv with semiempirical parameters ###\n",
    "    parameter_file_dir = \"/home/maxim/Projects/git2/PYSEQM_dev/seqm/params/\"\n",
    "    \n",
    "    ### directory with training set ###\n",
    "    #dataset_path = \"/home/maxim/Projects/TESTS/pyseqm_tests/HIPNN_TRAIN/training_set/\" \n",
    "    dataset_path = \"/home/maxim/Projects/git2/PYSEQM_dev/examples/hipnn_train_reactive/combined_datasets/\" \n",
    "    \n",
    "    ### Prefix for arrays in folder ###\n",
    "    dataset_name = ''\n",
    "\n",
    "    ### folder with models and plots ###\n",
    "    netname = 'TEST1'\n",
    "    dirname = netname\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname)\n",
    "    else:\n",
    "        pass\n",
    "        #raise ValueError(\"Directory {} already exists!\".format(dirname))\n",
    "    os.chdir(dirname)\n",
    "\n",
    "    TAG = 0 #False (0): first run, True(n): continue\n",
    "\n",
    "    dtype=torch.float64\n",
    "    torch.set_default_dtype(dtype)\n",
    "    #device = torch.device('cuda')\n",
    "    DEVICE = 'cuda'\n",
    "    #DEVICE = 'cpu'\n",
    "\n",
    "\n",
    "    ### list of parameters to be learned ###\n",
    "    #\"\"\"\n",
    "    learned = [\n",
    "               'U_ss', 'U_pp',\n",
    "               'zeta_s', 'zeta_p',\n",
    "               'beta_s', \n",
    "               'beta_p',\n",
    "               'g_ss',\n",
    "               'g_sp', 'g_pp', 'g_p2',\n",
    "               'h_sp',\n",
    "               'alpha',\n",
    "            'Gaussian1_K', #'Gaussian2_K', #'Gaussian3_K','Gaussian4_K',\n",
    "            'Gaussian1_L', #'Gaussian2_L', #'Gaussian3_L','Gaussian4_L',\n",
    "            'Gaussian1_M', #'Gaussian2_M', #'Gaussian3_M','Gaussian4_M',\n",
    "          ]\n",
    "    #\"\"\"\n",
    "\n",
    "    \n",
    "    ### SEQM parameters ###\n",
    "    seqm_parameters = {\n",
    "                \"method\": \"PM6_SP\",  # AM1, MNDO, PM3#\n",
    "                \"scf_eps\": 20.0e-5,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                #\"scf_converger\": [0, 0.4],\n",
    "                \"scf_converger\": [1, 0.7, 0.95, 30], # converger used for scf loop\n",
    "                                           # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                           # [1], adaptive mixing\n",
    "                                           # [2], adaptive mixing, then pulay\n",
    "                \"sp2\": [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,[True, eps] or [False], eps for SP2 conve criteria\n",
    "                \"elements\": [0, 1, 6, 7, 8],\n",
    "                \"learned\": learned,  # parameterlist[method], #['U_ss'], # learned parameters name list, e.g ['U_ss']\n",
    "                \"parameter_file_dir\": parameter_file_dir + \"/\",  # file directory for other required parameters\n",
    "                \"pair_outer_cutoff\": 1.0e10,  # consistent with the unit on coordinates\n",
    "                \"scf_backward\": 2, # 0: Hellmann–Feynman theorem, 1: recursive formula, 2: backpropagate through SCF (needed for training)\n",
    "                \"Hf_flag\": False,\n",
    "                'UHF' : True, # use unrestricted HF\n",
    "                'BS' : True, \n",
    "                'HIPNN_automatic_doublet': True, # assign doublet state to molecules with odd number of electrons\n",
    "                }\n",
    "\n",
    "    # Log the output of python to `training_log.txt`\n",
    "    with hippynn.tools.log_terminal(\"training_log_tag_%d.txt\" % TAG,'wt'):# and torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "        ### Hyperparameters for the network ###\n",
    "        network_params = {\n",
    "            \"possible_species\": [0,1,6,7,8],   # Z values of the elements\n",
    "            'n_features': 64,                     # Number of neurons at each layer\n",
    "            \"n_sensitivities\": 18,                # Number of sensitivity functions in an interaction layer\n",
    "            \"dist_soft_min\": 0.65,  # qm7 1.7  qm9 .85  AL100 .85\n",
    "            \"dist_soft_max\": 2.8,  # qm7 10.  qm9 5.   AL100 5.\n",
    "            \"dist_hard_max\": 3.6,  # qm7 15.  qm9 7.5  AL100 7.5\n",
    "            \"n_interaction_layers\": 2,            # Number of interaction blocks #2\n",
    "            \"n_atom_layers\": 3,                   # Number of atom layers in an interaction block #3\n",
    "        }\n",
    "\n",
    "\n",
    "        ### Define a model ###\n",
    "\n",
    "        species = inputs.SpeciesNode(db_name=\"Z\")\n",
    "\n",
    "        positions = inputs.PositionsNode(db_name=\"R\")\n",
    "\n",
    "        network = networks.Hipnn(\"HIPNN_seqm\", (species, positions), module_kwargs = network_params)\n",
    "\n",
    "        n_target_peratom = len(seqm_parameters[\"learned\"])\n",
    "\n",
    "        decay_factor = 1e-4\n",
    "        par_atom = HChargeNode(\"SEQM_Atom_Params\",network,module_kwargs=dict(n_target=n_target_peratom,first_is_interacting=True))\n",
    "        with torch.no_grad():\n",
    "            for layer in par_atom.torch_module.layers:\n",
    "                layer.weight.data *= decay_factor\n",
    "                layer.bias.data *= decay_factor\n",
    "\n",
    "        seqm_par = par_atom.atom_charges\n",
    "\n",
    "        lenergy = SEQM_AllNode(\"SEQM_Energy\",(par_atom, positions, species),seqm_parameters, decay_factor = 1.0e-4)\n",
    "\n",
    "        molecule_energy = lenergy.mol_energy\n",
    "        #molecule_energy = lenergy.Etot_m_Eiso\n",
    "\n",
    "        gradient  = physics.GradientNode(\"gradients\", (molecule_energy, positions), sign=-1)\n",
    "\n",
    "        notconverged = lenergy.notconverged\n",
    "        scale = ScaleNode(\"Scale\", (notconverged,))\n",
    "\n",
    "        # gradient.db_name='Gradient_ev'\n",
    "        # molecule_energy.db_name=\"EtEi\"\n",
    "    \n",
    "        gradient.db_name='F_DFT'\n",
    "        #molecule_energy.db_name=\"EATOMIZ_DFT_no_outlier\"\n",
    "        molecule_energy.db_name=\"Et_mNONd3h4bias\"\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        mol_mask = SEQM_MolMaskNode(\"SEQM_MolMask\", notconverged)\n",
    "        atom_mask = AtomMaskNode(\"Atom_Mask\", species)\n",
    "        gradient_pred = SEQM_MaskOnMolAtomNode(\"SEQM_MaskMolAtom_Pred\", (gradient, mol_mask, atom_mask)).pred\n",
    "        gradient_true = SEQM_MaskOnMolAtomNode(\"SEQM_MaskMolAtom_True\", (gradient.true, mol_mask.pred, atom_mask.pred))\n",
    "\n",
    "        molecule_energy_pred = SEQM_MaskOnMolNode(\"SEQM_MaskMol_Pred\", (molecule_energy, mol_mask)).pred\n",
    "        molecule_energy_true = SEQM_MaskOnMolNode(\"SEQM_MaskMol_True\", (molecule_energy.true, mol_mask.pred))\n",
    "\n",
    "\n",
    "        ### define loss quantities ###\n",
    "\n",
    "        rmse_gradient = loss.MSELoss(gradient_pred, gradient_true) ** (1./2.)\n",
    "        rmse_mol_energy = loss.MSELoss(molecule_energy_pred, molecule_energy_true) ** (1. / 2.)\n",
    "\n",
    "        #rmse_energy_grad = rmse_gradient + rmse_mol_energy\n",
    "\n",
    "        mae_gradient = loss.MAELoss(gradient_pred, gradient_true)\n",
    "        mae_mol_energy = loss.MAELoss(molecule_energy_pred, molecule_energy_true)\n",
    "\n",
    "        #mae_energy_grad = mae_gradient + mae_mol_energy\n",
    "        \n",
    "        rsq_gradient = loss.Rsq(gradient_pred, gradient_true)\n",
    "        rsq_mol_energy = loss.Rsq(molecule_energy_pred, molecule_energy_true)\n",
    "\n",
    "        ### SLIGHTLY MORE ADVANCED USAGE\n",
    "\n",
    "        #pred_per_atom = physics.PerAtom(\"PeratomPredicted\",(molecule_energy,species)).pred\n",
    "        #true_per_atom = physics.PerAtom(\"PeratomTrue\",(molecule_energy.true,species.true))\n",
    "\n",
    "        pred_per_atom1 = physics.PerAtom(\"PeratomPredicted\",(molecule_energy,species))\n",
    "        true_per_atom1 = physics.PerAtom(\"PeratomTrue\",(molecule_energy.true,species.true))\n",
    "        pred_per_atom = SEQM_MaskOnMolNode(\"SEQM_PerAtom_Pred\", (pred_per_atom1, mol_mask)).pred\n",
    "        true_per_atom = SEQM_MaskOnMolNode(\"SEQM_PerAtom_True\", (true_per_atom1.pred, mol_mask.pred))\n",
    "        mae_per_atom = loss.MAELoss(pred_per_atom,true_per_atom)\n",
    "        rmse_per_atom = loss.MSELoss(pred_per_atom,true_per_atom) ** (1. / 2.)\n",
    "\n",
    "        rmse_par = loss.MeanSq(seqm_par.pred)\n",
    "\n",
    "        ### END SLIGHTLY MORE ADVANCED USAGE\n",
    "\n",
    "        #loss_error = (rmse_energy_grad + mae_energy_grad) + rmse_par*8.0\n",
    "        #loss_error = (0.3*(rmse_gradient + mae_gradient) + (rmse_mol_energy + mae_mol_energy)) + rmse_par*5.0\n",
    "        #loss_error = (0.5*(rmse_gradient + mae_gradient) + rmse_mol_energy + mae_mol_energy) + rmse_par*10.0\n",
    "        #loss_error = ((mae_gradient + 0.3*rmse_gradient)) #+ rmse_par*1.0\n",
    "        loss_error = rmse_gradient + mae_gradient + 0.2*(rmse_mol_energy + mae_mol_energy) + 10.0*rmse_par #+ 0.2*(mae_mol_energy) #+ 0.5*rmse_par\n",
    "\n",
    "        #rbar = loss.Mean.of_node(hierarchicality)\n",
    "        l2_reg = loss.l2reg(network)\n",
    "        \n",
    "        loss_regularization = 1.0e-6 * loss.Mean(l2_reg) #+ rbar    # L2 regularization and hierarchicality regularization\n",
    "\n",
    "        train_loss = loss_error*scale.pred + loss_regularization\n",
    "\n",
    "        # Validation losses are what we check on the data between epochs -- we can only train to\n",
    "        # a single loss, but we can check other metrics too to better understand how the model is training.\n",
    "        # There will also be plots of these things over time when training completes.\n",
    "        validation_losses = {\n",
    "            \"TperAtom RMSE\":rmse_per_atom,\n",
    "            \"TperAtom MAE\" : mae_per_atom,\n",
    "            \"Force-RMSE\"   : rmse_gradient,\n",
    "            \"Force-MAE\"    : mae_gradient,\n",
    "            \"Force-RSQ\"    : rsq_gradient,\n",
    "            \"MolEn-RMSE\"   : rmse_mol_energy,\n",
    "            \"MolEn-MAE\"    : mae_mol_energy,\n",
    "            \"MolEn-RSQ\"    : rsq_mol_energy,\n",
    "            \"L2Reg\"        : l2_reg,\n",
    "            \"Loss-Err\"     : loss_error,\n",
    "            \"Loss-Reg\"     : loss_regularization,\n",
    "            \"Loss\"         : train_loss,\n",
    "        }\n",
    "        early_stopping_key = \"Loss-Err\"\n",
    "\n",
    "\n",
    "\n",
    "        plot_maker = plotting.PlotMaker(\n",
    "            # Simple plots which compare the network to the database\n",
    "\n",
    "            #plotting.Hist2D.compare(molecule_energy, saved=True),\n",
    "            plotting.Hist2D(molecule_energy_true, molecule_energy_pred,\n",
    "                            xlabel=\"True EtEi\",ylabel=\"Predicted EtEi\",\n",
    "                            saved=\"EtEi.png\"),\n",
    "            plotting.Hist2D(gradient_true, gradient_pred,\n",
    "                            xlabel=\"True Force\",ylabel=\"Predicted Force\",\n",
    "                            saved=\"grad.png\"),\n",
    "\n",
    "            #Slightly more advanced control of plotting!\n",
    "            plotting.Hist2D(true_per_atom,pred_per_atom,\n",
    "                            xlabel=\"True Energy/Atom\",ylabel=\"Predicted Energy/Atom\",\n",
    "                            saved=\"PerAtomEn.png\"),\n",
    "\n",
    "            #plotting.HierarchicalityPlot(hierarchicality.pred,\n",
    "            #                             molecule_energy.pred - molecule_energy.true,\n",
    "            #                             saved=\"HierPlot.pdf\"),\n",
    "            plot_every=1,   # How often to make plots -- here, epoch 0, 10, 20...\n",
    "        )\n",
    "\n",
    "        if TAG==0: #TRAINING FROM SCRATCH\n",
    "\n",
    "\n",
    "            training_modules, db_info = \\\n",
    "                assemble_for_training(train_loss,validation_losses,plot_maker=plot_maker)\n",
    "            training_modules[0].print_structure()\n",
    "\n",
    "    # ----------------- #\n",
    "    # Step 3: RUN MODEL #\n",
    "    # ----------------- #\n",
    "\n",
    "            database_params = {\n",
    "                'name': dataset_name,                            # Prefix for arrays in folder\n",
    "                'directory': dataset_path,\n",
    "                'quiet': False,                           # Quiet==True: suppress info about loading database\n",
    "                'seed': 1,                       # Random seed for data splitting\n",
    "                #'test_size': 0.1,                # Fraction of data used for testing\n",
    "                #'valid_size':0.1,\n",
    "                **db_info                 # Adds the inputs and targets names from the model as things to load\n",
    "            }\n",
    "\n",
    "\n",
    "            database = DirectoryDatabase(**database_params)\n",
    "            \n",
    "            ### a fraction of the data set to ignore (i.e., 0.9 means to ignore 90% of the data set and use 10% for train/test/validation) ###\n",
    "            database.make_random_split(\"ignore\",0.7)\n",
    "            del database.splits['ignore']\n",
    "            database.make_trainvalidtest_split(test_size=0.1,valid_size=0.1)\n",
    "\n",
    "            #from hippynn.pretraining import set_e0_values\n",
    "            #set_e0_values(henergy,database,energy_name=\"T_transpose\",trainable_after=False)\n",
    "\n",
    "            init_lr = 1.0e-4\n",
    "            optimizer = torch.optim.Adam(training_modules.model.parameters(),lr=init_lr)\n",
    "\n",
    "\n",
    "\n",
    "            scheduler =  RaiseBatchSizeOnPlateau(optimizer=optimizer,\n",
    "                                                max_batch_size=60,\n",
    "                                                patience=4,\n",
    "                                                factor=0.5\n",
    "                                                )\n",
    "\n",
    "            controller = PatienceController(optimizer=optimizer,\n",
    "                                            scheduler=scheduler,\n",
    "                                            batch_size=60,\n",
    "                                            eval_batch_size=60,\n",
    "                                            max_epochs=200,\n",
    "                                            termination_patience=20,\n",
    "                                            fraction_train_eval=0.1,\n",
    "                                            stopping_key=early_stopping_key,\n",
    "                                            )\n",
    "\n",
    "            scheduler.set_controller(controller)\n",
    "\n",
    "            experiment_params = hippynn.experiment.SetupParams(\n",
    "                controller = controller,\n",
    "                device=DEVICE,\n",
    "            )\n",
    "            print(experiment_params)\n",
    "\n",
    "            # Parameters describing the training procedure.\n",
    "\n",
    "            training_modules, controller, metric_tracker  = setup_training(training_modules=training_modules,\n",
    "                                                            setup_params=experiment_params)\n",
    "            \n",
    "        if TAG>0: #CONTINUE INTERRUPTED TRAINING\n",
    "            from hippynn.experiment.serialization import load_checkpoint_from_cwd, load_checkpoint\n",
    "            from hippynn.experiment import train_model\n",
    "            #load best model\n",
    "            #structure = load_checkpoint_from_cwd()\n",
    "            \n",
    "            #load last model\n",
    "            structure = load_checkpoint(\"experiment_structure.pt\", \"best_checkpoint.pt\")\n",
    "            training_modules = structure[\"training_modules\"]\n",
    "            training_modules[0].print_structure()\n",
    "            database = structure[\"database\"]\n",
    "            \n",
    "            ### a fraction of the data set to ignore (i.e., 0.9 means to ignore 90% of the data set and use 10% for train/test/validation) ###\n",
    "            database.make_random_split(\"ignore\",0.5)\n",
    "            del database.splits['ignore']\n",
    "            database.make_trainvalidtest_split(test_size=0.1,valid_size=0.1)\n",
    "            #controller = structure[\"controller\"]\n",
    "            \n",
    "            init_lr = 2.0e-5\n",
    "            optimizer = torch.optim.Adam(training_modules.model.parameters(),lr=init_lr)\n",
    "            \n",
    "            scheduler =  RaiseBatchSizeOnPlateau(optimizer=optimizer,\n",
    "                                                max_batch_size=80,\n",
    "                                                patience=3,\n",
    "                                                factor=0.5)\n",
    "            \n",
    "            controller = PatienceController(optimizer=optimizer,\n",
    "                                            scheduler=scheduler,\n",
    "                                            batch_size=80,\n",
    "                                            eval_batch_size=70,\n",
    "                                            max_epochs=200,\n",
    "                                            termination_patience=20,\n",
    "                                            fraction_train_eval=0.1,\n",
    "                                            stopping_key=early_stopping_key,\n",
    "                                            )\n",
    "            \n",
    "            metric_tracker = structure[\"metric_tracker\"]\n",
    "    \n",
    "    from hippynn.experiment import train_model\n",
    "    \n",
    "    store_all_better=True\n",
    "    store_best=True\n",
    "    if isinstance(training_modules[0], torch.nn.DataParallel):\n",
    "        seqm_module = training_modules[0].module.node_from_name('SEQM_Energy').torch_module\n",
    "    else:\n",
    "        seqm_module = training_modules[0].node_from_name('SEQM_Energy').torch_module\n",
    "    callbacks = [update_scf_eps(seqm_module, 0.9),\n",
    "                    save_and_stop_after(training_modules, controller, metric_tracker, store_all_better, store_best, [2,0,0,0])]\n",
    "    \n",
    "    train_model(training_modules=training_modules,\n",
    "                database=database,\n",
    "                controller=controller,\n",
    "                metric_tracker=metric_tracker,\n",
    "                callbacks=callbacks,batch_callbacks=None,\n",
    "                store_all_better=store_all_better,\n",
    "                store_best=store_best)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3d8a0-7ca7-477f-bd4a-c981b0b79efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipnn_1",
   "language": "python",
   "name": "hipnn_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
