{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be74f84-f719-43aa-b265-f09ee8050eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- #\n",
    "# to save output in log file #\n",
    "# ---------------------------- #\n",
    "##############################################\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"train1.log\", \"w+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 20\n",
    "##############################################\n",
    "\n",
    "\n",
    "# ---------------- #\n",
    "# Imported Modules #\n",
    "# ---------------- #\n",
    "import os\n",
    "import sys\n",
    "### path to PYSEQM ###\n",
    "sys.path.insert(1, '.../PYSEQM_dev/')\n",
    "\n",
    "### path to HIPNN ###\n",
    "sys.path.append('.../hippynn/')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from hippynn.interfaces.pyseqm_interface.seqm_nodes import *\n",
    "from hippynn.interfaces.pyseqm_interface.callback import update_scf_eps, save_and_stop_after\n",
    "import hippynn.interfaces.pyseqm_interface\n",
    "import hippynn\n",
    "from hippynn.graphs import inputs, networks, targets, physics\n",
    "from hippynn.graphs import loss\n",
    "from hippynn import plotting\n",
    "from hippynn.databases import DirectoryDatabase\n",
    "from hippynn.experiment.assembly import assemble_for_training\n",
    "from hippynn.experiment.controllers import RaiseBatchSizeOnPlateau,PatienceController\n",
    "from hippynn.experiment import setup_training\n",
    "from hippynn.experiment import train_model\n",
    "import seqm\n",
    "from seqm.basics import parameterlist\n",
    "\n",
    "\n",
    "### keeps SCF loops silent ###\n",
    "seqm.seqm_functions.scf_loop.debug = False\n",
    "\n",
    "hippynn.interfaces.pyseqm_interface.check.debug = True\n",
    "\n",
    "### maximum allowed SCF iterations ###\n",
    "seqm.seqm_functions.scf_loop.MAX_ITER = 50\n",
    "\n",
    "# torch.cuda.set_device(0) # Don't try this if you want CPU training!\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "\n",
    "'''\n",
    "it is fine to see the following message below:\n",
    "\n",
    "Javascript Error: IPython is not defined\n",
    "\n",
    "Autosaving every 20 seconds\n",
    "\n",
    ".../lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "Decorating your function! <function KSA_XL_BOMD.one_step at 0x7fb9febfbe50>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d443d0-38f7-4938-80c8-aea14d6eee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PULL DATA SET. Don't run this cell if the data is already downloaded ###\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "url = 'https://figshare.com/ndownloader/articles/19640052/versions/1'\n",
    "dst = 'training_set/data.zip'\n",
    "urlretrieve(url, dst)\n",
    "\n",
    "dlDict = {\"training_set/EtEi.npy\":\"https://figshare.com/ndownloader/files/35845121\",\n",
    "          \"training_set/Gradient_ev.npy\":\"https://figshare.com/ndownloader/files/35845133\",\n",
    "          \"training_set/R.npy\":\"https://figshare.com/ndownloader/files/35845145\",\n",
    "          \"training_set/Z.npy\":\"https://figshare.com/ndownloader/files/35845163\"}\n",
    "for file in list(dlDict):\n",
    "    urlretrieve(dlDict[file],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ca5eb-6624-4df5-9841-b15ccd3261e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "current_dir = '.../PYSEQM_dev/examples/hipnn_training/'\n",
    "os.chdir(current_dir)\n",
    "\n",
    "def main():\n",
    "\n",
    "    ### directory csv with semiempirical parameters ###\n",
    "    parameter_file_dir = \".../PYSEQM_dev/seqm/params\"\n",
    "    \n",
    "    ### directory with training set ###\n",
    "    dataset_path = \".../PYSEQM_dev/examples/hipnn_training/training_set/\" \n",
    "    \n",
    "    ### Prefix for arrays in folder ###\n",
    "    dataset_name = ''\n",
    "\n",
    "    ### folder with models and plots ###\n",
    "    netname = 'TEST1'\n",
    "    dirname = netname\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname)\n",
    "    else:\n",
    "        pass\n",
    "        #raise ValueError(\"Directory {} already exists!\".format(dirname))\n",
    "    os.chdir(dirname)\n",
    "\n",
    "    TAG = 0 #False (0): first run, True(n): continue\n",
    "\n",
    "    dtype=torch.float64\n",
    "    torch.set_default_dtype(dtype)\n",
    "    device = torch.device('cuda')\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "\n",
    "    ### list of parameters to be learned ###\n",
    "    #\"\"\"\n",
    "    learned = ['U_ss', 'U_pp',\n",
    "               'zeta_s', 'zeta_p',\n",
    "               #'beta_s', \n",
    "               'beta_p',\n",
    "               #'g_ss',\n",
    "               'g_sp', 'g_pp', 'g_p2', 'h_sp',\n",
    "               #'alpha',\n",
    "           # 'Gaussian1_K', 'Gaussian2_K', #'Gaussian3_K','Gaussian4_K',\n",
    "           # 'Gaussian1_L', 'Gaussian2_L', #'Gaussian3_L','Gaussian4_L',\n",
    "           # 'Gaussian1_M', 'Gaussian2_M', #'Gaussian3_M','Gaussian4_M',\n",
    "          ]\n",
    "    #\"\"\"\n",
    "\n",
    "    \n",
    "    ### SEQM parameters ###\n",
    "    seqm_parameters = {\n",
    "                \"method\": \"PM3\",  # AM1, MNDO, PM3#\n",
    "                \"scf_eps\": 27.2114e-5,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                \"scf_converger\": [1, 0.1], # converger used for scf loop\n",
    "                                           # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                           # [1], adaptive mixing\n",
    "                                           # [2], adaptive mixing, then pulay\n",
    "                \"sp2\": [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,[True, eps] or [False], eps for SP2 conve criteria\n",
    "                \"elements\": [0, 1, 6, 7, 8],\n",
    "                \"learned\": learned,  # parameterlist[method], #['U_ss'], # learned parameters name list, e.g ['U_ss']\n",
    "                \"parameter_file_dir\": parameter_file_dir + \"/\",  # file directory for other required parameters\n",
    "                \"pair_outer_cutoff\": 1.0e10,  # consistent with the unit on coordinates\n",
    "                \"scf_backward\": 2, # 0: Hellmannâ€“Feynman theorem, 1: recursive formula, 2: backpropagate through SCF (needed for training)\n",
    "                'UHF' : False, # use unrestricted HF\n",
    "                }\n",
    "\n",
    "    # Log the output of python to `training_log.txt`\n",
    "    with hippynn.tools.log_terminal(\"training_log_tag_%d.txt\" % TAG,'wt'):# and torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "        ### Hyperparameters for the network ###\n",
    "        network_params = {\n",
    "            \"possible_species\": [0,1,6,7,8],   # Z values of the elements\n",
    "            'n_features': 128,                     # Number of neurons at each layer\n",
    "            \"n_sensitivities\": 32,                # Number of sensitivity functions in an interaction layer\n",
    "            \"dist_soft_min\": 0.6,  # qm7 1.7  qm9 .85  AL100 .85\n",
    "            \"dist_soft_max\": 5.0,  # qm7 10.  qm9 5.   AL100 5.\n",
    "            \"dist_hard_max\": 7.5,  # qm7 15.  qm9 7.5  AL100 7.5\n",
    "            \"n_interaction_layers\": 2,            # Number of interaction blocks\n",
    "            \"n_atom_layers\": 3,                   # Number of atom layers in an interaction block\n",
    "        }\n",
    "\n",
    "\n",
    "        ### Define a model ###\n",
    "\n",
    "        species = inputs.SpeciesNode(db_name=\"Z\")\n",
    "\n",
    "        positions = inputs.PositionsNode(db_name=\"R\")\n",
    "\n",
    "        network = networks.Hipnn(\"HIPNN_seqm\", (species, positions), module_kwargs = network_params)\n",
    "\n",
    "        n_target_peratom = len(seqm_parameters[\"learned\"])\n",
    "\n",
    "        decay_factor = 1.0e-3\n",
    "        par_atom = HChargeNode(\"SEQM_Atom_Params\",network,module_kwargs=dict(n_target=n_target_peratom,first_is_interacting=True))\n",
    "        with torch.no_grad():\n",
    "            for layer in par_atom.torch_module.layers:\n",
    "                layer.weight.data *= decay_factor\n",
    "                layer.bias.data *= decay_factor\n",
    "\n",
    "        seqm_par = par_atom.atom_charges\n",
    "\n",
    "        lenergy = SEQM_AllNode(\"SEQM_Energy\",(par_atom, positions, species),seqm_parameters, decay_factor = 1.0e-4)\n",
    "\n",
    "        molecule_energy = lenergy.Etot_m_Eiso\n",
    "\n",
    "        gradient  = physics.GradientNode(\"gradients\", (molecule_energy, positions), sign=+1)\n",
    "\n",
    "        notconverged = lenergy.notconverged\n",
    "        scale = ScaleNode(\"Scale\", (notconverged,))\n",
    "\n",
    "        gradient.db_name='Gradient_ev'\n",
    "        molecule_energy.db_name=\"EtEi\"\n",
    "\n",
    "\n",
    "        mol_mask = SEQM_MolMaskNode(\"SEQM_MolMask\", notconverged)\n",
    "        atom_mask = AtomMaskNode(\"Atom_Mask\", species)\n",
    "        gradient_pred = SEQM_MaskOnMolAtomNode(\"SEQM_MaskMolAtom_Pred\", (gradient, mol_mask, atom_mask)).pred\n",
    "        gradient_true = SEQM_MaskOnMolAtomNode(\"SEQM_MaskMolAtom_True\", (gradient.true, mol_mask.pred, atom_mask.pred))\n",
    "\n",
    "        molecule_energy_pred = SEQM_MaskOnMolNode(\"SEQM_MaskMol_Pred\", (molecule_energy, mol_mask)).pred\n",
    "        molecule_energy_true = SEQM_MaskOnMolNode(\"SEQM_MaskMol_True\", (molecule_energy.true, mol_mask.pred))\n",
    "\n",
    "\n",
    "        ### define loss quantities ###\n",
    "\n",
    "        rmse_gradient = loss.MSELoss(gradient_pred, gradient_true) ** (1./2.)\n",
    "        rmse_mol_energy = loss.MSELoss(molecule_energy_pred, molecule_energy_true) ** (1. / 2.)\n",
    "\n",
    "        rmse_energy_grad = rmse_gradient + rmse_mol_energy\n",
    "\n",
    "        mae_gradient = loss.MAELoss(gradient_pred, gradient_true)\n",
    "        mae_mol_energy = loss.MAELoss(molecule_energy_pred, molecule_energy_true)\n",
    "\n",
    "        mae_energy_grad = mae_gradient + mae_mol_energy\n",
    "        rsq_gradient = loss.Rsq(gradient_pred, gradient_true)\n",
    "        rsq_mol_energy = loss.Rsq(molecule_energy_pred, molecule_energy_true)\n",
    "\n",
    "        ### SLIGHTLY MORE ADVANCED USAGE\n",
    "\n",
    "        #pred_per_atom = physics.PerAtom(\"PeratomPredicted\",(molecule_energy,species)).pred\n",
    "        #true_per_atom = physics.PerAtom(\"PeratomTrue\",(molecule_energy.true,species.true))\n",
    "\n",
    "        pred_per_atom1 = physics.PerAtom(\"PeratomPredicted\",(molecule_energy,species))\n",
    "        true_per_atom1 = physics.PerAtom(\"PeratomTrue\",(molecule_energy.true,species.true))\n",
    "        pred_per_atom = SEQM_MaskOnMolNode(\"SEQM_PerAtom_Pred\", (pred_per_atom1, mol_mask)).pred\n",
    "        true_per_atom = SEQM_MaskOnMolNode(\"SEQM_PerAtom_True\", (true_per_atom1.pred, mol_mask.pred))\n",
    "        mae_per_atom = loss.MAELoss(pred_per_atom,true_per_atom)\n",
    "\n",
    "        rmse_par = loss.MeanSq(seqm_par.pred)\n",
    "\n",
    "        ### END SLIGHTLY MORE ADVANCED USAGE\n",
    "\n",
    "        loss_error = (rmse_energy_grad + mae_energy_grad) + rmse_par*1.0\n",
    "\n",
    "        #rbar = loss.Mean.of_node(hierarchicality)\n",
    "        l2_reg = loss.l2reg(network)\n",
    "        loss_regularization = 1.0e-6 * loss.Mean(l2_reg) #+ rbar    # L2 regularization and hierarchicality regularization\n",
    "\n",
    "        train_loss = loss_error*scale.pred + loss_regularization\n",
    "\n",
    "        # Validation losses are what we check on the data between epochs -- we can only train to\n",
    "        # a single loss, but we can check other metrics too to better understand how the model is training.\n",
    "        # There will also be plots of these things over time when training completes.\n",
    "        validation_losses = {\n",
    "            \"TperAtom MAE\": mae_per_atom,\n",
    "            \"Force-RMSE\"  : rmse_gradient,\n",
    "            \"Force-MAE\"   : mae_gradient,\n",
    "            \"Force-RSQ\"   : rsq_gradient,\n",
    "            \"MolEn-RMSE\"  : rmse_mol_energy,\n",
    "            \"MolEn-MAE\"   : mae_mol_energy,\n",
    "            \"MolEn-RSQ\"   : rsq_mol_energy,\n",
    "            \"L2Reg\"       : l2_reg,\n",
    "            \"Loss-Err\"    : loss_error,\n",
    "            \"Loss-Reg\"    : loss_regularization,\n",
    "            \"Loss\"        : train_loss,\n",
    "        }\n",
    "        early_stopping_key = \"Loss-Err\"\n",
    "\n",
    "\n",
    "\n",
    "        plot_maker = plotting.PlotMaker(\n",
    "            # Simple plots which compare the network to the database\n",
    "\n",
    "            #plotting.Hist2D.compare(molecule_energy, saved=True),\n",
    "            plotting.Hist2D(molecule_energy_true, molecule_energy_pred,\n",
    "                            xlabel=\"True EtEi\",ylabel=\"Predicted EtEi\",\n",
    "                            saved=\"EtEi.png\"),\n",
    "            plotting.Hist2D(gradient_true, gradient_pred,\n",
    "                            xlabel=\"True Force\",ylabel=\"Predicted Force\",\n",
    "                            saved=\"grad.png\"),\n",
    "\n",
    "            #Slightly more advanced control of plotting!\n",
    "            plotting.Hist2D(true_per_atom,pred_per_atom,\n",
    "                            xlabel=\"True Energy/Atom\",ylabel=\"Predicted Energy/Atom\",\n",
    "                            saved=\"PerAtomEn.png\"),\n",
    "\n",
    "            #plotting.HierarchicalityPlot(hierarchicality.pred,\n",
    "            #                             molecule_energy.pred - molecule_energy.true,\n",
    "            #                             saved=\"HierPlot.pdf\"),\n",
    "            plot_every=1,   # How often to make plots -- here, epoch 0, 10, 20...\n",
    "        )\n",
    "\n",
    "        if TAG==0: #TRAINING FROM SCRATCH\n",
    "\n",
    "\n",
    "            training_modules, db_info = \\\n",
    "                assemble_for_training(train_loss,validation_losses,plot_maker=plot_maker)\n",
    "            training_modules[0].print_structure()\n",
    "\n",
    "    # ----------------- #\n",
    "    # Step 3: RUN MODEL #\n",
    "    # ----------------- #\n",
    "\n",
    "            database_params = {\n",
    "                'name': dataset_name,                            # Prefix for arrays in folder\n",
    "                'directory': dataset_path,\n",
    "                'quiet': False,                           # Quiet==True: suppress info about loading database\n",
    "                'seed': 8000,                       # Random seed for data splitting\n",
    "                #'test_size': 0.1,                # Fraction of data used for testing\n",
    "                #'valid_size':0.1,\n",
    "                **db_info                 # Adds the inputs and targets names from the model as things to load\n",
    "            }\n",
    "\n",
    "\n",
    "            database = DirectoryDatabase(**database_params)\n",
    "            \n",
    "            ### a fraction of the data set to ignore (i.e., 0.99 means to ignore 99% of the data set and use 1% for train/test/validation) ###\n",
    "            database.make_random_split(\"ignore\",0.99)\n",
    "            del database.splits['ignore']\n",
    "            database.make_trainvalidtest_split(test_size=0.1,valid_size=0.1)\n",
    "\n",
    "            #from hippynn.pretraining import set_e0_values\n",
    "            #set_e0_values(henergy,database,energy_name=\"T_transpose\",trainable_after=False)\n",
    "\n",
    "            init_lr = 0.5e-4\n",
    "            optimizer = torch.optim.Adam(training_modules.model.parameters(),lr=init_lr)\n",
    "\n",
    "\n",
    "\n",
    "            scheduler =  RaiseBatchSizeOnPlateau(optimizer=optimizer,\n",
    "                                                max_batch_size=64,\n",
    "                                                patience=5,\n",
    "                                                factor=0.5)\n",
    "\n",
    "            controller = PatienceController(optimizer=optimizer,\n",
    "                                            scheduler=scheduler,\n",
    "                                            batch_size=128,\n",
    "                                            eval_batch_size=128,\n",
    "                                            max_epochs=200,\n",
    "                                            termination_patience=20,\n",
    "                                            fraction_train_eval=0.1,\n",
    "                                            stopping_key=early_stopping_key,\n",
    "                                            )\n",
    "\n",
    "            scheduler.set_controller(controller)\n",
    "\n",
    "            experiment_params = hippynn.experiment.SetupParams(\n",
    "                controller = controller,\n",
    "                device=DEVICE,\n",
    "            )\n",
    "            print(experiment_params)\n",
    "\n",
    "            # Parameters describing the training procedure.\n",
    "\n",
    "            training_modules, controller, metric_tracker  = setup_training(training_modules=training_modules,\n",
    "                                                            setup_params=experiment_params)\n",
    "            \n",
    "        if TAG>0: #CONTINUE INTERRUPTED TRAINING\n",
    "            from hippynn.experiment.serialization import load_checkpoint_from_cwd, load_checkpoint\n",
    "            from hippynn.experiment import train_model\n",
    "            \n",
    "            #load best model\n",
    "            #structure = load_checkpoint_from_cwd()\n",
    "            \n",
    "            #load last model\n",
    "            structure = load_checkpoint(\"experiment_structure.pt\", \"last_checkpoint.pt\")\n",
    "            \n",
    "            training_modules = structure[\"training_modules\"]\n",
    "            \n",
    "            database = structure[\"database\"]\n",
    "            \n",
    "            ### a fraction of the data set to ignore (i.e., 0.99 means to ignore 99% of the data set and use 1% for train/test/validation) ###\n",
    "            database.make_random_split(\"ignore\",0.99)\n",
    "            del database.splits['ignore']\n",
    "            database.make_trainvalidtest_split(test_size=0.1,valid_size=0.1)\n",
    "            \n",
    "\n",
    "            \n",
    "            #controller = structure[\"controller\"]\n",
    "            \n",
    "            init_lr = 2e-5\n",
    "            optimizer = torch.optim.Adam(training_modules.model.parameters(),lr=init_lr)\n",
    "            \n",
    "            scheduler =  RaiseBatchSizeOnPlateau(optimizer=optimizer,\n",
    "                                                max_batch_size=64,\n",
    "                                                patience=5,\n",
    "                                                factor=0.5)\n",
    "            \n",
    "            controller = PatienceController(optimizer=optimizer,\n",
    "                                            scheduler=scheduler,\n",
    "                                            batch_size=128,\n",
    "                                            eval_batch_size=128,\n",
    "                                            max_epochs=200,\n",
    "                                            termination_patience=20,\n",
    "                                            fraction_train_eval=0.1,\n",
    "                                            stopping_key=early_stopping_key,\n",
    "                                            )\n",
    "            \n",
    "            metric_tracker = structure[\"metric_tracker\"]\n",
    "    \n",
    "    from hippynn.experiment import train_model\n",
    "    \n",
    "    store_all_better=True\n",
    "    store_best=True\n",
    "    if isinstance(training_modules[0], torch.nn.DataParallel):\n",
    "        seqm_module = training_modules[0].module.node_from_name('SEQM_Energy').torch_module\n",
    "    else:\n",
    "        seqm_module = training_modules[0].node_from_name('SEQM_Energy').torch_module\n",
    "    callbacks = [update_scf_eps(seqm_module, 0.9),\n",
    "                    save_and_stop_after(training_modules, controller, metric_tracker, store_all_better, store_best, [2,0,0,0])]\n",
    "    \n",
    "    train_model(training_modules=training_modules,\n",
    "                database=database,\n",
    "                controller=controller,\n",
    "                metric_tracker=metric_tracker,\n",
    "                callbacks=callbacks,batch_callbacks=None,\n",
    "                store_all_better=store_all_better,\n",
    "                store_best=store_best)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ac951-6e17-463a-9da6-9d5802c16b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a54c0b-4092-45ae-bea5-f5d95117c821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540b0c5-e53b-47f7-b6c5-3ce947f8acd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537b980-136a-4407-b1ce-88e3174b135c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipnn_1",
   "language": "python",
   "name": "hipnn_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
